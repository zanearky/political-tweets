---
title: "Organize Data Notebook"
---

```{r}
#| label: setup
#| include: false

library(here)
source(here("utils","check_packages.R"))
source(here("utils","functions.R"))
```

```{r}
#| label: read-data

# reading in biden data

tweets_biden <- read_csv("data/data_raw/tweets_csv/hashtag_joebiden.csv.gz",
                         na = "NA") |>
  mutate(created_at = ymd_hms(created_at),
         date_simple = format(created_at, "%m-%d"),
         only_hashtags = str_detect(tweet, "^#\\w+(\\s+#\\w+)*$"), # simple heuristic bot cleaning
         only_url = str_detect(tweet, "^https?://"),
         low_content = nchar(tweet) < 10,
         suspected_bot = only_hashtags | only_url | low_content) |>
  filter(!(is.na(date_simple)),
           !suspected_bot,
           str_count(tweet, boundary("word")) >5) |>
  select(tweet, 
         likes, 
         retweet_count,
         user_screen_name,
         user_followers_count,
         user_id,
         tweet_id,
         date_simple,
         created_at)

# reading in trump data
tweets_trump <- read_csv("data/data_raw/tweets_csv/hashtag_donaldtrump.csv.gz",
                         na = "NA") |>
  mutate(created_at = ymd_hms(created_at),
         date_simple = format(created_at, "%m-%d"),
         only_hashtags = str_detect(tweet, "^#\\w+(\\s+#\\w+)*$"), # simple heuristic bot cleaning
         only_url = str_detect(tweet, "^https?://"),
         low_content = nchar(tweet) < 10,
         suspected_bot = only_hashtags | only_url | low_content) |>
  filter(!(is.na(date_simple)),
           !suspected_bot,
           str_count(tweet, boundary("word")) >5) |>
  select(tweet, 
         likes, 
         retweet_count,
         user_screen_name,
         user_followers_count,
         user_id,
         tweet_id,
         date_simple,
         created_at)
```

```{r}
#| label: clean-data

# removing duplicates 
tweets_trump <- tweets_trump[!duplicated(tweets_trump$tweet_id),]
tweets_biden <- tweets_biden[!duplicated(tweets_biden$tweet_id),]

# create variables that capture addressivity markers like #'s and @'s
trump_sample <- tweets_trump |>
  distinct(tweet, .keep_all = TRUE) |>
  mutate(hashtags = str_extract_all(tweet, "(?<=\\s|^)#[A-Za-z0-9_]+"), # creating variables that capture
         mentions = str_extract_all(tweet, "@\\w+") # markers like #'s and @'s
         ) |>
  mutate(tweet = str_remove_all(tweet, "https?://\\S+"), # cleaning a variety of special characters within the tweet
         tweet = str_remove(tweet, "(\\s*#\\w+)+\\s*$"),
         tweet = str_remove_all(tweet, "@\\w+"), # remove @ and mention
         tweet = str_replace_all(tweet, "#(\\w+)", "\\1"), # remove only trailing hashtags
         tweet = str_remove_all(tweet, "[\U0001F600-\U0001F64F]"), # emojis
         tweet = str_remove_all(tweet, "[\U0001F300-\U0001F5FF]"), 
         tweet = str_remove_all(tweet, "[\U0001F680-\U0001F6FF]"), 
         tweet = str_remove_all(tweet, "[\U0001F1E0-\U0001F1FF]"),
         tweet = str_remove_all(tweet, "[^\\w\\s]"),
         tweet = str_squish(tweet)) |>
  mutate(language = detect_language(tweet)) |>
  filter(
    language == "en" # filtering tweets where language == english
  ) |>
  mutate(
    hashtags = sapply(hashtags, function (x) paste(x, collapse=" ")) |> 
      str_remove_all("#"), # we separate and collapse hashtags vector into long string
    mentions = sapply(mentions, function (x) paste(x, collapse=" ")) |>
         str_remove_all("@")) # we separate and collapse address vector into long string


biden_sample <- tweets_biden |>
  distinct(tweet, .keep_all = TRUE) |>
  mutate(hashtags = str_extract_all(tweet, "(?<=\\s|^)#[A-Za-z0-9_]+"),
         mentions = str_extract_all(tweet, "@\\w+")) |>
  mutate(tweet = str_remove_all(tweet, "https?://\\S+"),
         tweet = str_remove(tweet, "(\\s*#\\w+)+\\s*$"), 
         tweet = str_remove_all(tweet, "@\\w+"),
         tweet = str_replace_all(tweet, "#(\\w+)", "\\1"),
         tweet = str_remove_all(tweet, "[\U0001F600-\U0001F64F]"), 
         tweet = str_remove_all(tweet, "[\U0001F300-\U0001F5FF]"), 
         tweet = str_remove_all(tweet, "[\U0001F680-\U0001F6FF]"), 
         tweet = str_remove_all(tweet, "[\U0001F1E0-\U0001F1FF]"),
         tweet = str_remove_all(tweet, "[^\\w\\s]"),
         tweet = str_squish(tweet)) |>
  mutate(language = detect_language(tweet)) |>
  filter(
    language == "en"
  ) |>
  mutate(
    hashtags = sapply(hashtags, function (x) paste(x, collapse=" ")) |>
      str_remove_all("#"),
    mentions = sapply(mentions, function (x) paste(x, collapse=" ")) |>
         str_remove_all("@"))

# joining tweet datasets
list(`Tweets Trump` = unique(trump_sample$tweet_id),
     `Tweets Biden` = unique(biden_sample$tweet_id)) |>
  ggvenn(auto_scale = TRUE, fill_color = c("navy","seagreen"))

pol_tweets <- full_join(trump_sample, biden_sample)

pol_tweets <- pol_tweets[!duplicated(pol_tweets$tweet_id),]



# creating a sentiment variable

# tokenizing
tweets_tokens <- pol_tweets |>
  unnest_tokens(word, tweet)

# attributing sentiment
tweets_sentiment <- tweets_tokens |>
  inner_join(get_sentiments("bing"), by = "word", relationship = "many-to-many") |>
  count(tweet_id, sentiment) |>
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) |>
  mutate(sentiment = positive - negative)

pol_tweets <- pol_tweets |>
  left_join(tweets_sentiment, by = "tweet_id") |>
  mutate(sentiment = ifelse(is.na(sentiment), 0, sentiment),
         abs_sentiment = abs(sentiment),
         retweet_like_ratio = retweet_count / likes,
         retweet_like_ratio = ifelse((likes < 11 | retweet_count < 4) & retweet_like_ratio > 0.9, retweet_like_ratio * 0.1, retweet_like_ratio),
         hashtags_count = str_count(hashtags, regex("\\S+", ignore_case = TRUE))) |>
  select(user_id,
         user_screen_name,
         tweet_id,
         tweet,
         hashtags,
         mentions,
         likes,
         retweet_count,
         retweet_like_ratio,
         hashtags_count,
         user_followers_count,
         sentiment,
         abs_sentiment,
         created_at,
         date_simple,
         created_at) |>
  filter(likes > 0)

pol_tweets <- pol_tweets |> # filtering troublemakers
  filter(user_screen_name != "RareProject",
         user_screen_name != "_Goutham__") 

pol_tweets <- pol_tweets |>
  mutate(tweet = str_to_lower(tweet)) 


pol_tweets <- pol_tweets |> # removing duplicates
  distinct(tweet, .keep_all = TRUE)

pol_tweets <- pol_tweets |>
  mutate(tweet_id = as.character(tweet_id))

# subset into #democracy and #electionfraud groups

# quantile(pol_tweets$user_followers_count, 0.99, na.rm = TRUE) 
# top 10% of pol_tweets follower counts is 6843

# Fraud tags: Focus on delegitimizing the election (e.g., #Rigged, #Corruption).

# Democracy tags: Emphasize participation and legitimacy of the election system (e.g., #EveryVoteCounts).

fraud_tags <- c(
  "stopthesteal","electionfraud",
  "corruption","voterfraud", "fraud", "counteverylegalvote", "rigged"
)

dem_tags <- c(
  "counteveryvote",
  "countallthevotes",
  "democracy", 
  "everyvotecounts"
)

# Extract election fraud tweets
election_fraud <- pol_tweets |>
  filter(
    str_count(tweet, "\\S+") >= 5,
    str_detect(str_to_lower(tweet), regex("election.?fraud|voter.?fraud|rigged|corruption|steal.?election|dominion|\\bstop\\b[\\s-]the\\b[\\s-]steal\\b")) |
    str_detect(hashtags, regex(str_c(fraud_tags, collapse = "|"), ignore_case = TRUE))
  ) |>
  mutate(
    created_at = as.Date(created_at),
    sent_direction = factor(case_when(
      sentiment < 0 ~ "negative", 
      sentiment > 0 ~ "positive",
      sentiment == 0 ~ "neutral")),
    mentions = factor(case_when(
      str_detect(tweet, regex("trump", ignore_case = TRUE)) &
      str_detect(tweet, regex("biden", ignore_case = TRUE)) ~ "both",
      str_detect(tolower(tweet), "trump|realdonaldtrump|donaldtrump") ~ "trump",
      str_detect(tolower(tweet), "biden|joe|joebiden") ~ "biden",
      TRUE ~ "neither")),
    post_election = ifelse(created_at >= "2020-11-03", 1, 0),
    pop_account = ifelse(user_followers_count >= 6843, 1, 0),
    day_since_election = as.numeric(created_at - as.Date("2020-11-03"))
  ) |>
  filter(mentions != "both")

# Extract democracy tweets
democracy <- pol_tweets |>
  filter(
    str_count(tweet, "\\S+") >= 5,
    str_detect(str_to_lower(tweet), regex("democracy|vote.?early|\\bcount\\b[\\s-]every\\b[\\s-]vote\\b")) |
    str_detect(hashtags, regex(str_c(dem_tags, collapse = "|"), ignore_case = TRUE))
  ) |>
  mutate(
    created_at = as.Date(created_at),
    sent_direction = factor(case_when(
      sentiment < 0 ~ "negative", 
      sentiment > 0 ~ "positive",
      sentiment == 0 ~ "neutral")),
    mentions = factor(case_when(
      str_detect(tweet, regex("trump", ignore_case = TRUE)) &
      str_detect(tweet, regex("biden", ignore_case = TRUE)) ~ "both",
      str_detect(tolower(tweet), "trump|realdonaldtrump|donaldtrump") ~ "trump",
      str_detect(tolower(tweet), "biden|joe|joebiden") ~ "biden",
      TRUE ~ "neither")),
    post_election = ifelse(created_at >= "2020-11-03", 1, 0),
    pop_account = ifelse(user_followers_count >= 6843, 1, 0),
    day_since_election = as.numeric(created_at - as.Date("2020-11-03"))
  ) |>
  filter(mentions != "both")

# Get tweet_ids for each narrative
ef_ids <- unique(election_fraud$tweet_id)
dem_ids <- unique(democracy$tweet_id)

# Get overlapping tweet_ids in original full set (before filtering out "both")
all_ef_ids <- pol_tweets |>
  filter(
    str_count(tweet, "\\S+") >= 5,
    str_detect(str_to_lower(tweet), regex("election.?fraud|voter.?fraud|rigged|corruption|steal.?election|dominion|\\bstop\\b[\\s-]the\\b[\\s-]steal\\b")) |
    str_detect(hashtags, regex(str_c(fraud_tags, collapse = "|"), ignore_case = TRUE))
  ) |>
  pull(tweet_id)

all_dem_ids <- pol_tweets |>
  filter(
    str_count(tweet, "\\S+") >= 5,
    str_detect(str_to_lower(tweet), regex("democracy|vote.?early|\\bcount\\b[\\s-]every\\b[\\s-]vote\\b")) |
    str_detect(hashtags, regex(str_c(dem_tags, collapse = "|"), ignore_case = TRUE))
  ) |>
  pull(tweet_id)

# Find overlap ids
overlap_ids <- intersect(all_ef_ids, all_dem_ids)

# Create both dataset
both <- pol_tweets |>
  filter(tweet_id %in% overlap_ids) |>
  mutate(
    created_at = as.Date(created_at),
    sent_direction = factor(case_when(
      sentiment < 0 ~ "negative", 
      sentiment > 0 ~ "positive",
      sentiment == 0 ~ "neutral")),
    mentions = factor(case_when(
      str_detect(tweet, regex("trump", ignore_case = TRUE)) &
      str_detect(tweet, regex("biden", ignore_case = TRUE)) ~ "both",
      str_detect(tolower(tweet), "trump|realdonaldtrump|donaldtrump") ~ "trump",
      str_detect(tolower(tweet), "biden|joe|joebiden") ~ "biden",
      TRUE ~ "neither")),
    post_election = ifelse(created_at >= "2020-11-03", 1, 0),
    pop_account = ifelse(user_followers_count >= 6843, 1, 0),
    day_since_election = as.numeric(created_at - as.Date("2020-11-03"))
  )

# Finally, exclude overlapped tweets from election_fraud and democracy datasets

election_fraud <- election_fraud |>
  filter(!(tweet_id %in% overlap_ids))

democracy <- democracy |>
  filter(!(tweet_id %in% overlap_ids))

list(`democracy` = unique(democracy$tweet_id),
     `electionfraud` = unique(election_fraud$tweet_id)) |>
  ggvenn(auto_scale = TRUE, fill_color = c("navy","seagreen"))


democracy <- democracy |>
  mutate(engagement_weight = case_when(
           likes >= 234 | retweet_count >= 88 ~ 10,
          likes >= 26 | retweet_count >= 9 ~ 1.0,
           likes >= 11 | retweet_count >= 4 ~ 0.1,
           TRUE ~ 0.01
         ))


election_fraud <- election_fraud |>
  mutate(engagement_weight = case_when(
           likes >= 262 | retweet_count >= 126 ~ 10,
          likes >= 30 | retweet_count >= 12 ~ 1.0,
           likes >= 11 | retweet_count >= 4 ~ 0.1,
           TRUE ~ 0.01
         ))

election_fraud_log <- election_fraud |>
  filter(mentions == "biden" | mentions == "trump") |>
  mutate(mentions = relevel(mentions, ref = "biden"),
         mentions = ifelse(mentions == "trump", 0, 1),
         sent_direction = relevel(sent_direction, ref = "positive"))

democracy_log <- democracy |>
  filter(mentions == "biden" | mentions == "trump") |>
  mutate(mentions = relevel(mentions, ref = "trump"),
         mentions = ifelse(mentions == "biden", 0, 1))


# saving cleaned dataset
save(pol_tweets, file = here("data", "data_constructed", "political_tweets.RData"))
 save(election_fraud, file = here("data", "data_constructed", "election_fraud.RData"))
 save(democracy, file = here("data", "data_constructed", "democracy.RData"))
 
```


This quarto doc is used to organize the data.